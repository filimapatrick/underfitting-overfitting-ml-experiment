# ğŸ§¾ An Experimental Study of Underfitting and Overfitting in Supervised Machine Learning Using Polynomial Regression

## ğŸ¯ Aim

To understand how model complexity affects generalization performance in supervised machine learning.

## ğŸ¯ Objectives

- **Define and understand** underfitting and overfitting
- **Experimentally observe** the effect of model complexity
- **Analyze** training error vs validation error relationships
- **Understand** the biasâ€“variance tradeoff
- **Identify** the optimal model complexity for generalization

## ğŸ“Œ Problem Statement

In supervised machine learning, selecting an appropriate model complexity is critical for achieving good generalization performance.

**The Challenge:**
- **Too simple models** â†’ fail to capture underlying patterns (underfitting)
- **Too complex models** â†’ memorize noise instead of patterns (overfitting)

**Research Question:**
> How can we experimentally observe and quantify the effects of model complexity on model generalization?

## ğŸ§ª Methodology

### 1ï¸âƒ£ Data Generation
We generate synthetic nonlinear regression data using:

```
y = sin(x) + noise
```

- **Function**: Sinusoidal pattern to create a known nonlinear relationship
- **Noise**: Gaussian noise added to simulate real-world uncertainty
- **Purpose**: Controlled environment to study model behavior

### 2ï¸âƒ£ Data Splitting
The dataset is split into:

| Split | Purpose |
|-------|---------|
| **Training Set** | Model learning and parameter fitting |
| **Validation Set** | Generalization performance evaluation |

### 3ï¸âƒ£ Model Selection
We use **Polynomial Regression** which allows precise control over model complexity via polynomial degree.

| Degree Range | Expected Behavior |
|--------------|------------------|
| 1 | Underfitting (too simple) |
| 4â€“6 | Good fit (optimal complexity) |
| 15â€“20 | Overfitting (too complex) |

### 4ï¸âƒ£ Evaluation Metrics
We compute **Mean Squared Error (MSE)** for:
- Training error (performance on training data)
- Validation error (performance on unseen data)

### 5ï¸âƒ£ Analysis Approach
- **Visualization**: Plot regression curves for different complexities
- **Error Analysis**: Plot training vs validation error curves
- **Optimization**: Identify complexity where validation error is minimized
- **Theoretical**: Interpret results using biasâ€“variance decomposition

## ğŸ”„ Research Workflow

```mermaid
flowchart TD
    A[Start] --> B[Define Research Question]
    B --> C[Generate Synthetic Dataset]
    C --> D[Split Data: Train/Test]
    D --> E[Select Polynomial Degrees]
    E --> F[Train Model for Each Degree]
    F --> G[Compute Training Error]
    G --> H[Compute Validation Error]
    H --> I[Create Visualizations:<br/>â€¢ Regression Curves<br/>â€¢ Error vs Complexity]
    I --> J[Analyze Biasâ€“Variance Behavior]
    J --> K[Draw Conclusions]
    K --> L[End]
```

## ğŸ§  Theoretical Framework

### Bias-Variance Tradeoff
```
Generalization Error = BiasÂ² + Variance + Irreducible Noise
```

| Model Complexity | Bias | Variance | Result |
|------------------|------|----------|--------|
| **Low** (Simple) | High â†‘ | Low â†“ | Underfitting |
| **Optimal** | Balanced | Balanced | Good Generalization |
| **High** (Complex) | Low â†“ | High â†‘ | Overfitting |

### Expected Error Behavior

```
Model Complexity â†’

         Validation Error
              â†‘
              |     â•­â”€â•®
              |    â•±   â•²
              |   â•±     â•²
              |  â•±       â•²
              | â•±         â•²
   Training   |â•±___________â•²___
   Error      |             â•²
              |______________â•²____â†’
           Under-   Optimal   Over-
           fitting            fitting
```

## ğŸ“Š Expected Results

| Model State | Training Error | Validation Error | Bias | Variance | Interpretation |
|-------------|----------------|------------------|------|----------|----------------|
| **Underfitting** | High | High | High | Low | Cannot capture pattern |
| **Good Fit** | Low | Low | Balanced | Balanced | Optimal generalization |
| **Overfitting** | Very Low | High | Low | High | Memorizes training noise |

## ğŸ Expected Conclusions

1. **Optimal Complexity Exists**: There is a sweet spot that minimizes validation error
2. **U-shaped Validation Curve**: Validation error decreases then increases with complexity
3. **Generalization Gap**: Overfitting creates a large gap between training and validation performance
4. **Bias-Variance Tradeoff**: Confirms theoretical predictions about bias and variance behavior

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install numpy matplotlib scikit-learn
```

### Usage
```bash
# Clone or download the project
cd overfitting_ml

# Activate virtual environment
source .venv/bin/activate

# Run the experiment
python main.py
```

## ğŸ“ Project Structure
```
overfitting_ml/
â”œâ”€â”€ README.md
â”œâ”€â”€ .venv/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_generation.py
â”‚   â”œâ”€â”€ model_training.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ experiment_analysis.ipynb
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ plots/
â”‚   â””â”€â”€ metrics/
â””â”€â”€ requirements.txt
```

## ğŸ”¬ Key Learning Outcomes

- **Practical Understanding** of underfitting vs overfitting
- **Hands-on Experience** with model complexity tuning
- **Visualization Skills** for model evaluation
- **Theoretical Connection** between bias-variance tradeoff and real results

## ğŸ“š References

- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*
- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). *An Introduction to Statistical Learning*

---

**Author**: Patrick Filima  
**Date**: February 2026  
**Course**: Machine Learning Fundamentals# underfitting-overfitting-ml-experiment
